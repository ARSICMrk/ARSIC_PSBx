---
title: "La Cross Validation"
author: "Marko ARSIC / Rindra LUTZ"
date: "15/11/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **Introduction**

Le monde d’aujourd’hui est un monde connecté. Cette connectivité apporte son lot de d’informations diverses : ce sont les données, ou data en anglais. 

Pour traiter ces données, de nouveaux métiers ont vu le jour. Ces nouveaux métiers font appel à des traitements spécifiques dans le domaine des données comme par exemple la data analyse, la data science ou encore le datamining. 

Le Datamining regroupe des méthodes scientifiques destinées à l’exploration et l’analyse de données, à partir de grands volumes d’informations/de données, dans le but de créer de la valeur, comprendre des phénomènes, comprendre notre monde, et en particulier afin d’aider à prendre des décisions, anticiper des événements et agir.

Dans le Datamining, il existe principalement deux grandes familles de méthodes. Ce sont les méthodes descriptives et les méthodes prédictives.
  
  
### **Méthodes descriptives**

Une méthode descriptive (dite non supervisée) correspond à la recherche de structure, de relation, de corrélation. 

* Permet de mettre en évidence des informations non visibles simplement  
* Permet de résumer, synthétiser les données  
* Sans variable ou phénomène à expliquer a priori  

Ci-dessous, un tableau des méthodes descriptives.


![](D:\Contenu Clé USB\COURS\MSC - Bac +4 Bac +5\R\MethodesDesc.png)


### **Méthodes prédictives**

Une méthode prédictive (dite supervisée) correspond à la modélisation et la prédiction d’un phénomène donné.

* Permet de définir un pattern (un modèle/une relation) pour expliquer un événement  
* Permet d’extrapoler la cible  
* Avec une variable/un événement à expliquer  

Ci-dessous, un tableau des méthodes préditives.


![](D:\Contenu Clé USB\COURS\MSC - Bac +4 Bac +5\R\MethodesPred.png)

 
Traitant le sujet de la cross-validation, qui est une régression logistique, nous nous concentrerons sur les méthodes prédictives, et donc plus particulièrement sur les méthodes de régression.

La régression logistique est une technique prédictive. Elle vise à construire un modèle permettant de prédire/expliquer les valeurs prises par une variable cible qualitative (le plus souvent binaire, on parle alors de régression logistique binaire, et si elle possède plus de 2 modalités, on parle de régression logistique polytomique) à partir d’un ensemble de variables explicatives quantitatives ou qualitatives (un codage est nécessaire dans ce cas).  

Une fois que le modèle a été établi grâce à différents outils statistiques, il est alors nécessaire de valider sa fiabilité.  

**La cross validation pour mesurer la fiabilité du modèle**

Lors de toute modélisation, il est nécessaire de définir :  

• Une population d’apprentissage (Train) : pour entrainer le modèle  
• Une population de test (Test) : pour mesurer, tester la performance et robustesse du modèle 

Or, une véritable vérification via la cross validation demanderait de travailler ici avec 3 types d’échantillons :  

• Train  
• Valid  
• Test  

Cet échantillon complémentaire (Valid) permet par exemple de tester plusieurs modèles (en faisant varier les paramètres du modèle ou les variables) : on essaie plusieurs modèles sur le Train et on identifie le plus performant sur le Valid. On teste enfin sur l’échantillon de Test (totalement vierge) le pouvoir de généralisation/sur-apprentissage sur des données toutes fraîches.  

Il est possible de sophistiquer la structure des échantillons de Train/Validation au travers de quelques méthodes de validation croisée. En effet, les résultats dépendent de la manière dont ont été construits les 3 sous-ensembles Train/Valid/Test.  

Les 3 principales méthodes de validation croisée sont :  
• LOOV (leave-one-out cross-validation) (= LKOV avec k=1)
• LKOV (leave-k-out cross-validation)
• k-fold cross-validation


![](D:\Contenu Clé USB\COURS\MSC - Bac +4 Bac +5\R\Types_cross-validation.PNG)


La cross validation permet aussi de déterminer des paramètres du modèle : on met en compétition k « sous-modèles » dont on mesure la performance pour déterminer le paramètre testé dont la performance du modèle est la meilleure.  

**Le sur apprentissage : problématique majeure en modélisation** 

Un modèle trop complexe, intégrant trop d’inputs et « épousant » trop les données d’apprentissage amènera donc une très bonne performance sur l’échantillon d’apprentissage (par construction), mais aura trop appris, notamment les bruits ou cas aberrants.  
Il sera alors moins performant sur des données qui n’ont pas servi à la construction du modèle, c’est-à-dire sur les données sur lesquelles on souhaite faire la prédiction.  
L’enjeu est donc de trouver le bon niveau de sophistication pour obtenir un bon niveau de performance sur l’échantillon d’apprentissage et sur l’échantillon de test.  

Il n’y a pas sur-apprentissage lorsque la performance du modèle en Test est légèrement plus faible que celle en Train. Un écart trop grand est signe de **sur-apprentissage**.


```{r}
library(tidyverse)
library(caret)
# Load the data
data("swiss")
# Inspect the data
sample_n(swiss, 3)
# Split the data into training and test set
set.seed(123)
training.samples <- swiss$Fertility %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- swiss[training.samples, ]
test.data <- swiss[-training.samples, ]
# Build the model
model <- lm(Fertility ~., data = train.data)
# Make predictions and compute the R2, RMSE and MAE
predictions <- model %>% predict(test.data)
data.frame( R2 = R2(predictions, test.data$Fertility),
            RMSE = RMSE(predictions, test.data$Fertility),
            MAE = MAE(predictions, test.data$Fertility))  
            
RMSE(predictions, test.data$Fertility)/mean(test.data$Fertility)

# Define training control
set.seed(123) 
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
               trControl = train.control)
# Summarize the results
print(model)

# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
               trControl = train.control)
# Summarize the results
print(model)
```

